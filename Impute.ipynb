{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from os.path import isfile\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, r2_score\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(df_in, feature, model):\n",
    "    \n",
    "    # The rows which have a missing value in the feature\n",
    "    rows = (~df_in[feature].isnull())\n",
    "    \n",
    "    # Which columns to use for training\n",
    "    cols = df_in.drop(['subjid', 'visit', 'studyid', 'visdy', 'visstat', 'hdcat', 'seq'] + [feature], axis=1)\\\n",
    "                .dropna(axis='columns').columns\n",
    "    \n",
    "    # The labels\n",
    "    y = df_in.loc[rows,feature].values\n",
    "    \n",
    "    # Get training data\n",
    "    if model == 'knn':\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df_in.loc[:,cols].values)\n",
    "        X = scaler.transform(df_in.loc[rows,cols].values)\n",
    "    else:\n",
    "        X = df_in.loc[rows,cols].values\n",
    "    \n",
    "    # Get missing data\n",
    "    mhx_cols = ['hxtobcpd', 'hxtobyos', 'hxpacky', 'hxmarfrq', 'hxherfrq', 'hxcocfrq', 'hxclbfrq', 'hxampfrq',\n",
    "                'hxritfrq', 'hxhalfrq', 'hxinhfrq', 'hxopifrq', 'hxpakfrq', 'hxbarfrq', 'hxtrqfrq']\n",
    "    \n",
    "    # Feature is age related, keep only the first visit\n",
    "    if ('age' in feature or feature in ['rtrddur', 'sxfam', 'sxsubj', 'hddiagn']) or (feature in mhx_cols):\n",
    "        df_missing = df_in.loc[~rows,:].reset_index().groupby('subjid').first().reset_index()\n",
    "        df_missing.index = df_missing['index']\n",
    "        df_missing = df_missing.drop('index', axis=1)\n",
    "        \n",
    "        # Transform if Knn\n",
    "        if model == 'knn':\n",
    "            X_missing = scaler.transform(df_missing.loc[:,cols].values)\n",
    "        else:\n",
    "            X_missing = df_missing.loc[:,cols].values\n",
    "        y_missing = df_missing.loc[:,feature].values\n",
    "    # Keep all visits\n",
    "    else:\n",
    "        df_missing = df_in.loc[~rows,:]\n",
    "        if model == 'knn':\n",
    "            X_missing = scaler.transform(df_in.loc[~rows,cols].values)\n",
    "        else:\n",
    "            X_missing = df_in.loc[~rows,cols].values\n",
    "        y_missing = df_in.loc[~rows,feature].values\n",
    "    \n",
    "    return X, y, X_missing, df_missing.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model(model, problem):\n",
    "    if model == 'lin':\n",
    "        if problem == 'reg' or problem == 'ordinal':\n",
    "            reg = LinearRegression(n_jobs=-1)\n",
    "        elif problem == 'class':\n",
    "            reg = LogisticRegression(n_jobs=-1, class_weight='balanced', random_state=42)\n",
    "    elif model == 'tree':\n",
    "        if problem == 'reg' or problem == 'ordinal':\n",
    "            reg = RandomForestRegressor(100, n_jobs=-1, random_state=42)\n",
    "        elif problem == 'class':\n",
    "            reg = RandomForestClassifier(100, n_jobs=-1, class_weight='balanced', random_state=42)\n",
    "    else:\n",
    "        if problem == 'reg'  or problem == 'ordinal':\n",
    "            reg = KNeighborsRegressor(n_neighbors=5, weights='distance', n_jobs=-1)\n",
    "        elif problem == 'class':\n",
    "            reg = KNeighborsClassifier(n_neighbors=5, weights='distance', n_jobs=-1)\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, feature, model='knn', problem='reg'):\n",
    "    \"\"\"\n",
    "    :param df_in: The df used to train the model on\n",
    "    :param feature: The column to imput\n",
    "    :param missing: Number of missing values in feature\n",
    "    :param n: \n",
    "    :param k: The number of K-folds for the cross validation\n",
    "    :param model: Name of model to use (lin, tree, knn)\n",
    "    :param problem: The prediction problem\n",
    "    :param verbose: Whether the model should return informative output\n",
    "    \n",
    "    Makes a training dataset and trains the given model on it using K-fold cross validation.\n",
    "    \n",
    "    return: train and test evaluation scores per k-fold, model, feature\n",
    "    \"\"\"        \n",
    "    # Create K-folds\n",
    "    kfold_models = []\n",
    "    train_stats = []\n",
    "    test_stats = []\n",
    "    i = 1\n",
    "    \n",
    "    k = 10\n",
    "    kf = KFold(n_splits=k, random_state=42, shuffle=True)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Choose model\n",
    "        reg = choose_model(model, problem)\n",
    "\n",
    "        # Train models on fold\n",
    "        try:\n",
    "            reg.fit(X_train, y_train)               \n",
    "        except:\n",
    "            print('err')\n",
    "            return np.zeros((10, 8)), np.zeros((10, 8))\n",
    "\n",
    "        # Add to list of models\n",
    "        kfold_models.append(reg)\n",
    "\n",
    "        # Get statistics of model\n",
    "        train_stats.append([feature, df.shape[0] - X.shape[0], model, problem, i] + \n",
    "                           evaluation(feature, np.mean(y), X_train, y_train, reg, problem))\n",
    "\n",
    "        test_stats.append([feature, df.shape[0] - X.shape[0], model, problem, i] + \n",
    "                          evaluation(feature, np.mean(y), X_test, y_test, reg, problem))\n",
    "        i+=1\n",
    "\n",
    "    return kfold_models, train_stats, test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_prediction(feature, pred):\n",
    "    if ((isinstance(pred, int) or isinstance(pred, float)) and \n",
    "        feature not in ['pf', 'rp', 'bp', 'gh', 'vt', 'sf', 're', 'mh', 'pcs', 'mcs']):\n",
    "        pred = np.array(pred)\n",
    "\n",
    "    if feature in ['hxmarfrq', 'hxherfrq', 'hxcocfrq', 'hxclbfrq', 'hxampfrq', 'hxritfrq', 'hxhalfrq', 'hxinhfrq',\n",
    "                     'hxopifrq', 'hxpakfrq', 'hxbarfrq', 'hxtrqfrq']:\n",
    "        # HX drug abuse\n",
    "        return np.round(pred, 0).astype(int).clip(1, 3)\n",
    "    elif feature in ['marfrq', 'herfrq', 'cocfrq', 'clbfrq', 'ampfrq', 'ritfrq', 'halfrq', 'inhfrq', 'opifrq',\n",
    "                     'pakfrq', 'barfrq', 'trqfrq']:\n",
    "        # Drug abuse\n",
    "        return np.round(pred, 0).astype(int).clip(1, 3)\n",
    "    elif feature in ['ocularh', 'ocularv', 'sacinith', 'sacinitv', 'sacvelh', 'sacvelv', 'dysarth', 'tongue',\n",
    "                     'fingtapr', 'fingtapl', 'prosupr', 'prosupl', 'luria', 'rigarmr', 'rigarml', 'brady',\n",
    "                     'dysttrnk', 'dystrue', 'dystlue', 'dystrle', 'dystlle', 'chorface', 'chorbol', 'chortrnk',\n",
    "                     'chorrue', 'chorlue', 'chorrle', 'chorlle', 'gait', 'tandem', 'retropls', 'diagconf']:\n",
    "        # Motorscore 0-4\n",
    "        return np.round(pred, 0).astype(int).clip(0, 4)\n",
    "    elif feature in ['occupatn', 'finances', 'adl']:\n",
    "        # TFCscore 0-3\n",
    "        return np.round(pred, 0).astype(int).clip(0, 3)\n",
    "    elif feature in ['chores', 'carelevl']:\n",
    "        # TFCscore 0-2\n",
    "        return np.round(pred, 0).astype(int).clip(0, 2) \n",
    "    elif feature == 'indepscl':\n",
    "        # Fascore Transform to nearest 5 percentage\n",
    "        return (5 * np.round(pred.clip(0) / 5, 0)).astype(int)\n",
    "    elif feature in ['pbas1sv', 'pbas1fr', 'pbas1wo', 'pbas2sv', 'pbas2fr', 'pbas2wo', 'pbas3sv', 'pbas3fr',\n",
    "                     'pbas3wo', 'pbas4sv', 'pbas4fr', 'pbas4wo', 'pbas5sv', 'pbas5fr', 'pbas5wo', 'pbas6sv',\n",
    "                     'pbas6fr', 'pbas6wo', 'pbas7sv', 'pbas7fr', 'pbas7wo', 'pbas8sv', 'pbas8fr', 'pbas8wo',\n",
    "                     'pbas9sv', 'pbas9fr', 'pbas9wo', 'pbas10sv', 'pbas10fr', 'pbas10wo', 'pbas11sv', 'pbas11fr',\n",
    "                     'pbas11wo']:\n",
    "        return np.round(pred, 0).astype(int).clip(0, 4)\n",
    "    else:\n",
    "        # ints\n",
    "        return np.round(pred, 0).astype(int)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(feature, m, X, y, reg, problem):\n",
    "    \"\"\"\n",
    "    :param feature: which feature to tranform\n",
    "    :param m: the mean of the predictions\n",
    "    :param X: the input data\n",
    "    :param y: the labeled data\n",
    "    :param reg: the model to predict values\n",
    "    :param problem: the type of prediction: class, ordinal or reg\n",
    "    \n",
    "    Prediction is done and then the values are clipped and rounded to the correct format.\n",
    "    Format depdends on the feature that is predicted.\n",
    "    After that the function returns the r2/f1 score, the mae and the rmse\n",
    "    return:\n",
    "    \"\"\"\n",
    "    pred = clip_prediction(feature, reg.predict(X))\n",
    "    mean = clip_prediction(feature, m)\n",
    "\n",
    "    if problem == 'reg':\n",
    "        SS_tot = np.sum(np.power(y - mean, 2))\n",
    "        SS_res = np.sum(np.power(y - pred, 2))\n",
    "        score = 1 - (SS_res/SS_tot)\n",
    "    elif problem == 'class' or problem == 'ordinal':\n",
    "        score = f1_score(y, pred, average='weighted')\n",
    "    \n",
    "    if problem == 'ordinal':\n",
    "        # Only take wrong values\n",
    "        dif = y - pred\n",
    "        mae = np.mean(np.abs(dif[dif != 0]))\n",
    "        rmse = np.sqrt(np.mean(np.power(dif[dif != 0], 2)))\n",
    "    elif problem == 'class' or problem == 'reg':\n",
    "        mae = mean_absolute_error(y, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y, pred))\n",
    "    \n",
    "    return [score, mae, rmse]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/filtered_pre_and_manifest.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Divide categorical and ordinal columns </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['emplusl', 'emplany', 'volunt', 'fafinan', 'grocery', 'cash', 'supchild', 'drive', 'housewrk',\n",
    "            'laundry', 'prepmeal', 'telephon', 'ownmeds', 'feedself', 'dress', 'bathe', 'pubtrans', 'walknbr',\n",
    "            'walkfall', 'walkhelp', 'comb', 'trnchair', 'bed', 'toilet', 'carehome',\n",
    "           ]\n",
    "\n",
    "ord_cols = ['hxmarfrq', 'hxherfrq', 'hxcocfrq', 'hxclbfrq', 'hxampfrq', 'hxritfrq', 'hxhalfrq', 'hxinhfrq',\n",
    "            'hxopifrq', 'hxpakfrq', 'hxbarfrq', 'hxtrqfrq', # medical\n",
    "            'marfrq', 'herfrq', 'cocfrq', 'clbfrq', 'ampfrq', 'ritfrq', 'halfrq', 'inhfrq', 'opifrq',\n",
    "            'pakfrq', 'barfrq', 'trqfrq', # General 1\n",
    "            'ocularh', 'ocularv', 'sacinith', 'sacinitv', 'sacvelh', 'sacvelv', 'dysarth', 'tongue', 'fingtapr',\n",
    "            'fingtapl', 'prosupr', 'prosupl', 'luria', 'rigarmr', 'rigarml', 'brady', 'dysttrnk', 'dystrue',\n",
    "            'dystlue', 'dystrle', 'dystlle', 'chorface', 'chorbol', 'chortrnk', 'chorrue', 'chorlue', 'chorrle',\n",
    "            'chorlle', 'gait', 'tandem', 'retropls', 'diagconf', # Motor\n",
    "            'occupatn', 'finances', 'chores', 'adl', 'carelevl', # TFC\n",
    "            'indepscl', # Fascore\n",
    "            'pbas1sv', 'pbas1fr', 'pbas1wo', 'pbas2sv',\n",
    "            'pbas2fr', 'pbas2wo', 'pbas3sv', 'pbas3fr', 'pbas3wo', 'pbas4sv', 'pbas4fr', 'pbas4wo', 'pbas5sv',\n",
    "            'pbas5fr', 'pbas5wo', 'pbas6sv', 'pbas6fr', 'pbas6wo', 'pbas7sv', 'pbas7fr', 'pbas7wo', 'pbas8sv',\n",
    "            'pbas8fr', 'pbas8wo', 'pbas9sv', 'pbas9fr', 'pbas9wo', 'pbas10sv', 'pbas10fr', 'pbas10wo',\n",
    "            'pbas11sv', 'pbas11fr', 'pbas11wo', # PBA\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_imputation_models(df_in, ordered_features, old_results, impute):    \n",
    "    known_variables = old_results[0].variable.values.reshape(-1)\n",
    "    original_df = df_in.copy()\n",
    "    \n",
    "    old_new_model = []\n",
    "    \n",
    "    train_results = list(old_results[0].values)\n",
    "    test_results = list(old_results[1].values)\n",
    "    \n",
    "    pbar = tqdm(total=len(ordered_features))\n",
    "    for f in ordered_features:\n",
    "        # Skip features which have already been imputed\n",
    "        if f in known_variables:\n",
    "            pbar.set_description(\"Skipping...\")\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        \n",
    "        subset = df_in.copy()\n",
    "        \n",
    "        # Get correct problem\n",
    "        if f in cat_cols:\n",
    "            p = 'class'\n",
    "        elif f in ord_cols:\n",
    "            p = 'ordinal'\n",
    "        else:\n",
    "            p = 'reg'\n",
    "        \n",
    "        # Take subset of dataset if in features\n",
    "#         if 'age' in f or f in ['rtrddur', 'sxfam', 'sxsubj', 'hddiagn',\n",
    "#                                'hxtobcpd', 'hxtobyos', 'hxpacky', 'hxmarfrq', 'hxherfrq', 'hxcocfrq', 'hxclbfrq', 'hxampfrq',\n",
    "#                                'hxritfrq', 'hxhalfrq', 'hxinhfrq', 'hxopifrq', 'hxpakfrq', 'hxbarfrq', 'hxtrqfrq']:\n",
    "#             # \n",
    "#             subset = subset.loc[(subset[f] != 0) | (df_in[f].isnull())]\n",
    "            # subset = subset.drop_duplicates(['subjid']).groupby('subjid').head(1)\n",
    "        if f in ['alcunits', 'tobcpd', 'tobyos', 'packy', 'cafpd', 'sbh1n', 'sbh3n', 'sbh4n',\n",
    "                 'marfrq', 'herfrq', 'cocfrq', 'clbfrq', 'ampfrq', 'ritfrq', 'halfrq', 'inhfrq', 'opifrq',\n",
    "                 'pakfrq', 'barfrq', 'trqfrq', # General 1\n",
    "                ]:\n",
    "            # Ignore features where 0 means does not do it or nothing\n",
    "            subset = subset.loc[(subset[f] > 0)  | (df_in[f].isnull())]\n",
    "        \n",
    "        # If the number of not missing rows is smaller than the number of k-fold, continue\n",
    "        if (~subset[f].isnull()).sum() <= 10 or subset.shape[0] <= 10:\n",
    "            pbar.set_description(\"Skipping...\")\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        \n",
    "        # Train models\n",
    "        imputation_models = {}\n",
    "        for model in ['lin', 'tree', 'knn']:\n",
    "            pbar.set_description(\"Training {} model\".format(model))\n",
    "            input_data, labels, _, _ = extract_data(subset.copy(), f, model)\n",
    "            imputation_models[model], train_s, test_s = train_model(input_data, labels, f, model=model, problem=p)\n",
    "            for fold in range(10):\n",
    "                train_results.append(train_s[fold])\n",
    "                test_results.append(test_s[fold])\n",
    "\n",
    "        # Impute using best model\n",
    "        if impute:\n",
    "            mean_score = np.mean(np.array(test_results[-30:])[:,5].reshape(3,10).astype(float), axis=1)\n",
    "            m = np.argmax(mean_score)\n",
    "            \n",
    "            # Check if new model is better than previous model\n",
    "            if mean_score[m] < test_best1.loc[test_best1['variable'] == f, 'score'].mean():\n",
    "                model = test_best1.loc[test_best1['variable'] == f, 'model'].unique()[0]\n",
    "                print(f, model)\n",
    "                input_data, labels, missing_data, missing_index = extract_data(original_df.copy(), f, model)\n",
    "                imputation_models[model], train_s, test_s = train_model(input_data, labels, f, model=model, problem=p)\n",
    "                pbar.set_description(\"Imputing \" + f + \", using older \" + model + \" model\")\n",
    "                old_new_model.append([f, model, 1])\n",
    "            else:\n",
    "                _, _, missing_data, missing_index = extract_data(subset.copy(), f, model)\n",
    "                # Get correct model\n",
    "                if m == 0:\n",
    "                    model = 'lin'\n",
    "                    pbar.set_description(\"Imputing \" + f + \", using linear model\")\n",
    "                    # val, pred, idx_mis, size = imp_missing(subset, f, model='lin', problem=p, verbose=False)\n",
    "                elif m == 1:\n",
    "                    model = 'tree'\n",
    "                    pbar.set_description(\"Imputing \" + f + \", using tree model\")\n",
    "                    # val, pred, idx_mis, size = imp_missing(subset, f, model='tree', problem=p, verbose=False)\n",
    "                else:\n",
    "                    model = 'knn'\n",
    "                    pbar.set_description(\"Impute \" + f + \", using Knn\")\n",
    "                    # val, pred, idx_mis, size = imp_missing(subset, f, model='knn', problem=p, verbose=False)\n",
    "                old_new_model.append([f, model, 2])\n",
    "\n",
    "            pred = np.array([imputer.predict(missing_data) for imputer in imputation_models[model]]).mean(axis=0)\n",
    "            pred = clip_prediction(f, pred)\n",
    "\n",
    "            # if 'age' in f or f in ['rtrddur', 'sxfam', 'sxsubj', 'hddiagn']:\n",
    "            #     df_in = imp_age(df_in.copy(), f, missing_index, pred)\n",
    "            if 'age' in f or f in ['rtrddur', 'sxfam', 'sxsubj', 'hxtobcpd', 'hxtobyos', 'hxpacky',\n",
    "                                   'hxmarfrq', 'hxherfrq', 'hxcocfrq', 'hxclbfrq', 'hxampfrq','hxritfrq',\n",
    "                                   'hxhalfrq', 'hxinhfrq', 'hxopifrq', 'hxpakfrq', 'hxbarfrq', 'hxtrqfrq']:\n",
    "                df_in = imp_mhx(df_in.copy(), f, missing_index, pred)\n",
    "            else:\n",
    "                df_in.loc[missing_index,f] = pred\n",
    "        # End loop\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Save best model per round\n",
    "    if impute:\n",
    "        pd.DataFrame(old_new_model, columns=['variable', 'model', 'round']).to_csv('tables/best_models.csv', index=False)\n",
    "    \n",
    "    train_results = pd.DataFrame(train_results, columns=['variable', 'missing', 'model',\n",
    "                                                         'problem', 'fold', 'score', 'mae', 'rmse'])\n",
    "    test_results = pd.DataFrame(test_results, columns=['variable', 'missing', 'model',\n",
    "                                                       'problem', 'fold', 'score', 'mae', 'rmse'])\n",
    "    if impute:\n",
    "        return df_in, train_results, test_results\n",
    "    else:\n",
    "        return train_results, test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation round 1 (evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Load results round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use older training results if they are available\n",
    "if isfile('tables/impute_train_1.csv'):\n",
    "    train_results1 = pd.read_csv('tables/impute_train_1.csv')\n",
    "else:\n",
    "    train_results1 = pd.DataFrame([], columns=['variable', 'missing', 'model', 'problem', 'fold', 'score', 'mae', 'rmse'])\n",
    "\n",
    "if isfile('tables/impute_test_1.csv'):\n",
    "    test_results1 = pd.read_csv('tables/impute_test_1.csv')\n",
    "else:\n",
    "    test_results1 = pd.DataFrame([], columns=['variable', 'missing', 'model', 'problem', 'fold', 'score', 'mae', 'rmse'])\n",
    "    \n",
    "print(train_results1.shape)\n",
    "print(test_results1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to reset results\n",
    "# test_results1 = pd.DataFrame([], columns=['variable', 'missing', 'model', 'problem', 'fold', 'score', 'mae', 'rmse'])\n",
    "# train_results1 = pd.DataFrame([], columns=['variable', 'missing', 'model', 'problem', 'fold', 'score', 'mae', 'rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Get missing features and remove features which can be calculated by using other variables </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis = df.isnull().sum().sort_values(ascending=True).replace(0, np.nan).dropna()\\\n",
    "        .drop(['motscore', 'fascore', 'tfcscore',\n",
    "               'depscore', 'irascore', 'psyscore', 'aptscore', 'exfscore',\n",
    "               'hxpacky', 'packy',\n",
    "               # 'hddiagn',\n",
    "              ], axis='index')\n",
    "mis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Evaluate imputation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results1, test_results1 = train_imputation_models(df.copy(), mis.index,\n",
    "                                                        (train_results1, test_results1),\n",
    "                                                        impute=False\n",
    "                                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine old and new results, if needed\n",
    "# train_results1 = to_df(train_results1, new_train_results1, 'train_1')\n",
    "# test_results1 = to_df(test_results1, new_test_results1, 'test_1')\n",
    "if True:\n",
    "    train_results1.to_csv('tables/impute_train_1.csv', index=False)\n",
    "    test_results1.to_csv('tables/impute_test_1.csv', index=False)\n",
    "else:\n",
    "    print('Not saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Get best models per variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_models(scores):\n",
    "    \"\"\"\n",
    "    :param scores: Performance of the imputation models\n",
    "    \n",
    "    Calculate the best models based on the mean r2/f1 score of the folds per feature\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mean score per variable and model\n",
    "    means = scores.groupby([\"variable\", \"model\"]).mean().reset_index()\n",
    "    # Models per variable with highest mean score\n",
    "    models = means.loc[means.groupby('variable').score.idxmax(), :]\n",
    "    \n",
    "    # Get values from all the best scores\n",
    "    rows = []\n",
    "    for i, row in enumerate(models.sort_values('score', ascending=False)[['variable', 'model']].values):\n",
    "        for r in scores[(scores[['variable', 'model']].isin(row)).all(1)].values:\n",
    "            rows.append(r)\n",
    "    \n",
    "    out = pd.DataFrame(rows,\n",
    "                       columns=['variable', 'missing', 'model', 'problem', 'fold', 'score', 'mae', 'rmse'],\n",
    "                      )\n",
    "    out = out.astype({'variable': str, 'missing': int, 'model': str, 'problem': str, 'fold': int,\n",
    "                      'score': float, 'mae': float, 'rmse': float})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_best1 = best_models(train_results1.dropna())\n",
    "test_best1 = best_models(test_results1.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2 Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imp_age(df_in, f, idx_mis, pred):\n",
    "    \"\"\"\n",
    "    Only the first visit is imputed\n",
    "    After that recalculate the correct duration at the next visits\n",
    "    In the next visit the patient might be 0 years older or 1+ years older\n",
    "    \"\"\"\n",
    "    # Add predictions to first visits for each subject\n",
    "    df_in.loc[idx_mis,f] = pred\n",
    "    for i in idx_mis:\n",
    "        # Get the predicted age variable (x), subject (s) and age (a) in first visit\n",
    "        x = df_in.loc[i,f]\n",
    "        s = df_in.loc[i,'subjid']\n",
    "        a = df_in.loc[i, 'age']\n",
    "        visit = 1\n",
    "        # Check if next index is still the same subject\n",
    "        while s == df_in.loc[i + visit,'subjid']:\n",
    "            # Take age from visit\n",
    "            a_new = df_in.loc[i+visit, 'age']\n",
    "            # Duration = prediction + (current age - first visit age)\n",
    "            df_in.loc[i+visit,f] = x + (a_new - a)\n",
    "            # Next index/visit\n",
    "            visit += 1\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imp_mhx(df_in, f, idx_mis, pred):\n",
    "    \"\"\"\n",
    "    Impute baseline variables.\n",
    "    Only on the first visit a prediction is made\n",
    "    Fill in the other visits using prediction at first visit\n",
    "    \"\"\"\n",
    "    df_in.loc[idx_mis,f] = pred\n",
    "    for i in idx_mis:\n",
    "        # Get prediction at first visit\n",
    "        x = df_in.loc[i,f]\n",
    "        # Get subject at first visit\n",
    "        s = df_in.loc[i,'subjid']\n",
    "        visit = 1\n",
    "        # If the next row is still the same subject, impute using prediction at first visit\n",
    "        while s == df_in.loc[i + visit,'subjid']:\n",
    "            df_in.loc[i+visit,f] = x\n",
    "            visit += 1\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Load results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isfile('tables/impute_train_2.csv'):\n",
    "    train_results2 = pd.read_csv('tables/impute_train_2.csv')\n",
    "else:\n",
    "    train_results2 = pd.DataFrame([], columns=['variable', 'missing', 'model', 'problem', 'fold', 'score', 'mae', 'rmse'])\n",
    "\n",
    "if isfile('tables/impute_test_2.csv'):\n",
    "    test_results2 = pd.read_csv('tables/impute_test_2.csv')\n",
    "else:\n",
    "    test_results2 = pd.DataFrame([], columns=['variable', 'missing', 'model', 'problem', 'fold', 'score', 'mae', 'rmse'])\n",
    "    \n",
    "print(train_results2.shape)\n",
    "print(test_results2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to reset results\n",
    "train_results2 = pd.DataFrame([], columns=['variable', 'missing', 'model', 'problem', 'fold', 'score', 'mae', 'rmse'])\n",
    "test_results2 = pd.DataFrame([], columns=['variable', 'missing', 'model', 'problem', 'fold', 'score', 'mae', 'rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Impute in order of test_best1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.copy()\n",
    "new_df, train_results2, test_results2 = train_imputation_models(new_df, test_best1.variable.unique(),\n",
    "                                                                (train_results2, test_results2),\n",
    "                                                                impute=True,\n",
    "                                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    train_results2.to_csv('tables/impute_train_2.csv', index=False)\n",
    "    test_results2.to_csv('tables/impute_test_2.csv', index=False)\n",
    "else:\n",
    "    print('Not saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Get best model per variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_best2 = best_models(train_results2.dropna())\n",
    "test_best2 = best_models(test_results2.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,df.isnull().any()].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[:,new_df.isnull().any()].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Recalculate variables with imputed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['packy'] = ((new_df['tobcpd'] * 0.05) * new_df['tobyos']).round(1)\n",
    "new_df['hxpacky'] = ((new_df['hxtobcpd'] * 0.05) * new_df['hxtobyos']).round(1)\n",
    "\n",
    "new_df['motscore'] = new_df[['ocularh', 'ocularv', 'sacinith', 'sacinitv', 'sacvelh', 'sacvelv', 'dysarth', 'tongue',\n",
    "                             'fingtapr', 'fingtapl', 'prosupr', 'prosupl', 'luria', 'rigarmr', 'rigarml', 'brady',\n",
    "                             'dysttrnk', 'dystrue', 'dystlue', 'dystrle', 'dystlle', 'chorface', 'chorbol', 'chortrnk',\n",
    "                             'chorrue', 'chorlue', 'chorrle', 'chorlle', 'gait', 'tandem', 'retropls', 'diagconf']].sum(axis=1)\n",
    "\n",
    "new_df['tfcscore'] = new_df[['occupatn', 'finances', 'chores', 'adl', 'carelevl']].sum(axis=1)\n",
    "\n",
    "new_df['fascore'] = new_df[['emplusl', 'emplany', 'volunt', 'fafinan', 'grocery', 'cash', 'supchild', 'drive', 'housewrk',\n",
    "                            'laundry', 'prepmeal', 'telephon', 'ownmeds', 'feedself', 'dress', 'bathe', 'pubtrans', 'walknbr',\n",
    "                            'walkfall', 'walkhelp', 'comb', 'trnchair', 'bed', 'toilet', 'carehome']].sum(axis=1)\n",
    "\n",
    "new_df['depscore'] = ((new_df['pbas1sv'] * new_df['pbas1fr']) + (new_df['pbas2sv'] * new_df['pbas2fr']) + \n",
    "                      (new_df['pbas3sv'] * new_df['pbas3fr']))\n",
    "\n",
    "new_df['irascore'] = (new_df['pbas4sv'] * new_df['pbas4fr']) + (new_df['pbas5sv'] * new_df['pbas5fr'])\n",
    "\n",
    "new_df['psyscore'] = (new_df['pbas9sv'] * new_df['pbas9fr']) + (new_df['pbas10sv'] * new_df['pbas10fr'])\n",
    "\n",
    "new_df['aptscore'] = (new_df['pbas6sv'] * new_df['pbas6fr'])\n",
    "\n",
    "new_df['exfscore'] = (new_df['pbas7sv'] * new_df['pbas7fr']) + (new_df['pbas8sv'] * new_df['pbas8fr'])\n",
    "\n",
    "new_df['dbscore'] = (new_df['pbas11sv'] * new_df['pbas11fr'])\n",
    "\n",
    "# Take mean, couldnt be imputed\n",
    "new_df['sbh1n'] = int(round(df['sbh1n'], 0).mean())\n",
    "new_df['sbh3n'] = int(round(df['sbh1n'], 0).mean())\n",
    "new_df['sbh4n'] = int(round(df['sbh1n'], 0).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Remove remaining missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[:,new_df.isnull().any()].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.drop(new_df.loc[:,new_df.isnull().any()].columns, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Save imputed dataset </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('data/imputed_pre_and_manifest.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
