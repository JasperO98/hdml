{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from os.path import join, isfile, isdir\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from models.RNN import RNN\n",
    "\n",
    "from plot import Plotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here different labels can be selected to show performance\n",
    "\n",
    "pred = ['drive']\n",
    "prob = ['classification']\n",
    "out = join('output', '_'.join(pred), 'tuning')\n",
    "\n",
    "# Name of measure labels\n",
    "if prob[0] == 'regression':\n",
    "    measure_labels = ['test_{}_{}'.format(pred[0], m) for m in ['MAE', 'RMSE', 'Max AE', 'R2']]\n",
    "else:\n",
    "    measure_labels = ['test_{}_{}'.format(pred[0], m) for m in ['AUC', 'F1', 'Accuracy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve logs and performance (metrics appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make one big pd dataframe of the tuned arguments and models\n",
    "# This makes the appendix of all the variable metrics\n",
    "\n",
    "performance = pd.DataFrame([])\n",
    "values = []\n",
    "\n",
    "# Retrieve log for each model type\n",
    "for model_type in ['GRU_RNN']: #  ['RandomForestClass', 'RandomForestReg', 'LinearSVMClass', 'LinearSVMReg', 'Constant', 'GRU_RNN', 'NN', 'LSTM_RNN', '1d_CNN', 'SimpleRNN_RNN']:\n",
    "    # Model performances\n",
    "    if not isdir(join(out, model_type)):\n",
    "        print(join(out, model_type), 'not found')\n",
    "        continue\n",
    "    # Read log\n",
    "    log = pd.read_csv(join(out, model_type, 'log.csv')) # , usecols=['model'] + measure_labels)\n",
    "    log['model_type'] = model_type\n",
    "    # Concatenate log with df\n",
    "    performance = pd.concat([performance, log])\n",
    "    \n",
    "    # Loop through all trained models\n",
    "    for trained_models in glob.glob(join(out, model_type, 'model*')):\n",
    "        for model in trained_models.split('\\n'):\n",
    "            model_name = os.path.split(model)[-1]\n",
    "            if isfile(join(model, 'train_args.pickle')):\n",
    "                # Get training parameters of model to change some columns names\n",
    "                # Also to show which different hyperparameters are tuned\n",
    "                model_args = pickle.load(open(join(model, 'train_args.pickle'), 'rb'))\n",
    "                \n",
    "                # If recurrent specify which one SimpleRNN, LSTM, or GRU\n",
    "                if 'recurrent_layer' in model_args.keys():\n",
    "                    model_args['model_type'] = model_args['recurrent_layer'].__name__ + '_RNN'\n",
    "                    del(model_args['recurrent_layer'])\n",
    "                else:\n",
    "                    model_args['model_type'] = model_type\n",
    "                \n",
    "                # Change n_filters and hiddensize parameter to combined column (same thing)\n",
    "                if 'n_filters' in model_args.keys():\n",
    "                    model_args['hidden_size/n_filters'] = model_args['n_filters']\n",
    "                    del(model_args['n_filters'])\n",
    "                elif 'hidden_size' in model_args.keys():\n",
    "                    model_args['hidden_size/n_filters'] = model_args['hidden_size']\n",
    "                    del(model_args['hidden_size'])\n",
    "            \n",
    "            # Baseline model (no training args)\n",
    "            else:\n",
    "                model_args = {'model_type': model_type}\n",
    "            \n",
    "            model_args['model'] = model_name\n",
    "            values.append(model_args)\n",
    "\n",
    "# Show pandas dataframe of trained models evaluation metrics and the information in the log file\n",
    "df = pd.DataFrame(values)\n",
    "df = df.merge(performance, on=['model', 'model_type'])\n",
    "df = df.rename(columns={'model': 'model_dir'})\n",
    "# df = df.loc[df['layers'] != 5] # Remove layer=5, only trained for some\n",
    "# df = df.drop(['model'], axis='columns')\n",
    "# df = df.loc[:, ((~df.isnull().all()) & (df.nunique() != 1))]\n",
    "\n",
    "# Rename model type to more interpretaable name\n",
    "type_to_name = {'RandomForestClass': 'RF', 'RandomForestReg': 'RF',\n",
    "                'LinearSVMClass': 'SVM', 'LinearSVMReg': 'SVM', 'Constant': 'Constant', 'NN': 'NN',\n",
    "                'GRU_RNN': 'GRU', 'LSTM_RNN': 'LSTM', '1d_CNN': 'CNN', 'SimpleRNN_RNN': 'SimpleRNN'}\n",
    "df['model'] = df['model_type'].apply(lambda x: type_to_name[x])\n",
    "\n",
    "df.groupby('model').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Make the appendix </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort the big dataframe by the first measure label (MAE/AUC) to define best model\n",
    "stats_all_models = df.sort_values(by=measure_labels[0])\\\n",
    "        .loc[:, ~df.columns.isin(['model_type', 'model_dir'])]\n",
    "\n",
    "# Reformat floats to .3 precision\n",
    "stats_all_models['L2'] = stats_all_models['L2'].map('{:.0e}'.format)\n",
    "# stats_all_models['layers'] = stats_all_models['layers'].map('{:g}'.format)\n",
    "stats_all_models['hidden_size/n_filters'] = stats_all_models['hidden_size/n_filters'].map('{:g}'.format)\n",
    "if 'n_estimators' in stats_all_models.columns:\n",
    "    stats_all_models['n_estimators'] = stats_all_models['n_estimators'].map('{:g}'.format)\n",
    "if 'epsilon' in stats_all_models.columns:\n",
    "    stats_all_models['epsilon'] = stats_all_models['epsilon'].map('{:.0e}'.format)\n",
    "\n",
    "# For each measure (MAE, RMSE, etc.) reformat precision of float to .3\n",
    "# Also change column names\n",
    "for m in measure_labels:\n",
    "    stats_all_models[m] = stats_all_models[m].map('{:.3f}'.format)\n",
    "    new_m = m.split('_')\n",
    "    new_m[0] = 'train'\n",
    "    new_m = '_'.join(new_m)\n",
    "    stats_all_models[new_m] = stats_all_models[new_m].map('{:.3f}'.format)\n",
    "    stats_all_models = stats_all_models.rename(columns={m: 'test ' + m.split('_')[-1],\n",
    "                                                        new_m: 'train ' + new_m.split('_')[-1]})\n",
    "\n",
    "# Replace nans with -, looks nicer\n",
    "stats_all_models = stats_all_models.replace(np.nan, '-')\n",
    "stats_all_models = stats_all_models.replace('nan', '-')\n",
    "# Drop some uninformative info\n",
    "stats_all_models = stats_all_models.drop(['epochs', 'best epoch', 'elapsed time'], axis='columns')\n",
    "\n",
    "# Sort the dataframe by model, test MAE for regression\n",
    "# Sort the dataframe by model, test AUC, test F1, test Accuracy\n",
    "stats_all_models = stats_all_models.sort_values(by=['model', 'test MAE'] if prob[0] == 'regression'  \n",
    "                                                    else ['model', 'test AUC', 'test F1', 'test Accuracy'],\n",
    "                                                ascending=(True if prob[0] == 'regression' else False)\n",
    "                                               )\n",
    "stats_all_models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save appendix\n",
    "stats_all_models.to_csv('tables/all_models_{}.csv'.format(pred[0]), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define x labels and layout plot\n",
    "if prob[0] == 'classification':\n",
    "    col_measures = ['test AUC', 'test F1', 'test Accuracy']\n",
    "    rows, cols = 2, 2\n",
    "else:\n",
    "    col_measures = ['test MAE', 'test RMSE', 'test Max AE', 'test R2']\n",
    "    rows, cols = 2, 2\n",
    "\n",
    "# Color dict to sort the different models\n",
    "custom_dict = {'CNN': 0, 'LSTM': 1, 'GRU': 2, 'SimpleRNN': 3, 'NN': 4, 'RF': 5, 'SVM': 6, 'Constant': 7}\n",
    "\n",
    "# Define figures and axes\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(5.972, 8.67 * .5), sharex=True, sharey=False)\n",
    "\n",
    "# Loop through each different measure\n",
    "for i, m, ax in zip(range(len(col_measures)), col_measures, axs.reshape(-1)):\n",
    "    stats_all_models[m] = stats_all_models[m].astype(float)\n",
    "    # Barplot the measure m in ax i\n",
    "    sns.barplot(x='model', y=m, hue='model', dodge=False,\n",
    "                data=stats_all_models.sort_values(by=['model'], key=lambda x: x.map(custom_dict)),\n",
    "                ci=100, errwidth=1.5, capsize=.05,\n",
    "                estimator=min if m != 'test R2' and prob[0] == 'regression' else max,\n",
    "                ax=ax)\n",
    "    ax.get_legend().remove()\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel(' '.join(m.split(' ')[1:]))\n",
    "    \n",
    "    # Set x and y lim\n",
    "    min_val = np.inf\n",
    "    max_val = -np.inf\n",
    "    for p in ax.patches:\n",
    "        val = p.get_height()\n",
    "        if val < min_val:\n",
    "            min_val = val\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "    ax.set_ylim((min_val * 0.98, max_val * 1.01))\n",
    "\n",
    "# Remove last axes (empty)\n",
    "if prob[0] == 'classification':\n",
    "    axs.reshape(-1)[-1].remove()\n",
    "\n",
    "# Add legend and save plots\n",
    "axs.reshape(-1)[1].legend(loc=7, bbox_to_anchor=(1.8, 0.3), prop={\"size\":8})\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(right=0.8)\n",
    "plt.savefig('output/figures/best_models_{}.pdf'.format(pred[0]))\n",
    "plt.savefig('output/figures/best_models_{}.png'.format(pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Time Steps + Status Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Load data for RNN and other models </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('data/reshaped/PDS5_IRP_{}.pickle'.format(pred[0]), 'rb'))\n",
    "RNN_data = pickle.load(open('data/reshaped/PDS5_IRP_RNN_{}.pickle'.format(pred[0]), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load all tuned models using the wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Use dictionary to use propper model wrapper with model name\n",
    "if prob[0] == 'regression':\n",
    "    name_to_model = {'RF': RandomForestReg, 'SVM': LinearSVMReg, 'Constant': Constant, 'GRU': RNN, 'LSTM': RNN,\n",
    "                     'CNN': CNN, 'NN': NN, 'SimpleRNN': RNN}\n",
    "else:\n",
    "    name_to_model = {'RF': RandomForestClass, 'SVM': LinearSVMClass, 'Constant': Constant, 'GRU': RNN,\n",
    "                     'LSTM': RNN, 'CNN': CNN, 'NN': NN, 'SimpleRNN': RNN}\n",
    "    \n",
    "model_to_name = dict((v,k) for k, v in name_to_model.items())\n",
    "\n",
    "# Load all models using the model wrappers\n",
    "all_models = []\n",
    "for name, model_dir, model in zip(df['model_type'].values, df['model_dir'].values,\n",
    "                                  df['model'].values):\n",
    "    # RNN\n",
    "    if name in ['SimpleRNN', 'LSTM', 'GRU']:\n",
    "        all_models.append(name_to_model[model](data=RNN_data, outdir=join(out, name, model_dir), load_from_dir=True))\n",
    "    # Other ML algorithms\n",
    "    elif name != 'Constant':\n",
    "        all_models.append(name_to_model[model](data=data, outdir=join(out, name, model_dir), load_from_dir=True))\n",
    "    # Constant\n",
    "    else:\n",
    "        all_models.append(Constant(data=data, outdir=out, load_from_dir=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate performance per time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values = []\n",
    "\n",
    "# Loop through all models\n",
    "for mod in all_models:\n",
    "    # Load plotter\n",
    "    if mod.__class__.__name__ == 'RNN':\n",
    "        plotter = Plotter(data=RNN_data, model=mod, outdir=mod.outdir, save_mode=False, show_mode=False)\n",
    "        name = mod.recurrent_layer.__name__\n",
    "    else:\n",
    "        name = mod.__class__.__name__\n",
    "        plotter = Plotter(data=data, model=mod, outdir=mod.outdir, save_mode=False, show_mode=False)\n",
    "    \n",
    "    # Get the evaluation metrics per time step\n",
    "    for p in pred:\n",
    "        values = plotter.forecasting_measures('test', p).values\n",
    "        values = np.hstack([values, np.vstack([name] * values.shape[0])])\n",
    "        all_values.append(values)\n",
    "del(plotter)\n",
    "\n",
    "# Create pandas dataframe of evaluation metrics\n",
    "performance_timesteps = pd.DataFrame(np.concatenate(all_values),\n",
    "                                     columns=['n', 'time step (t)', 'xlabel', 'metric', 'value', 'model'])\n",
    "performance_timesteps = performance_timesteps.replace(to_replace=r'RandomForest.+', value='RF', regex=True)\n",
    "performance_timesteps = performance_timesteps.replace(to_replace=r'.+SVM.+', value='SVM', regex=True)\n",
    "performance_timesteps['value'] = performance_timesteps['value'].astype(float)\n",
    "performance_timesteps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Plot performance per time step </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get correct metrics\n",
    "if prob[0] == 'regression':\n",
    "    measures = ['MAE', 'RMSE', 'Max AE', 'R2']\n",
    "else:\n",
    "    measures = ['AUC', 'F1', 'Accuracy']\n",
    "\n",
    "# Create dictionary to order the models in the same way\n",
    "custom_dict = {'CNN': 0, 'LSTM': 1, 'GRU': 2, 'SimpleRNN': 3, 'NN': 4, 'RF': 5, 'SVM': 6, 'Constant': 7}\n",
    "\n",
    "# Define figure and axes\n",
    "fig, axs = plt.subplots(len(measures), 1, figsize=(5.972, 8.67 * .8), sharex=True, sharey=False)\n",
    "for i, m, ax in zip(range(len(measures)), measures, axs):\n",
    "    # Barplot each measure (m) at axis i\n",
    "    sns.barplot(x='xlabel', y='value', hue='model',\n",
    "                data=performance_timesteps.loc[performance_timesteps['metric']==m]\\\n",
    "                                          .sort_values(by=['model', 'time step (t)'],\n",
    "                                                       key=lambda x: x.map(custom_dict)),\n",
    "                ci=100, errwidth=1.5, capsize=.05,\n",
    "                estimator=min if m != 'R2' and prob[0] == 'regression' else max,\n",
    "                ax=ax)\n",
    "    ax.get_legend().remove()\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel(m)\n",
    "    \n",
    "    # Set x and y lim\n",
    "    min_val = np.inf\n",
    "    max_val = -np.inf\n",
    "    for p in ax.patches:\n",
    "        val = p.get_height()\n",
    "        if val < min_val:\n",
    "            min_val = val\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "    ax.set_ylim((min_val * 0.98, max_val * 1.01))\n",
    "\n",
    "# Add legend and save figure\n",
    "axs[1].legend(loc=7, bbox_to_anchor=(1.3, 0.5), prop={\"size\":8})\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(right=0.8)\n",
    "plt.savefig('results/figures/temporal_performance_{}.pdf'.format(pred[0]))\n",
    "plt.savefig('results/figures/temporal_performance_{}.png'.format(pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate performance per status group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values = []\n",
    "\n",
    "# Loop through all models\n",
    "for mod in all_models:\n",
    "    # Use the plotter wrapper to get the measures\n",
    "    if mod.__class__.__name__ == 'RNN':\n",
    "        plotter = Plotter(data=RNN_data, model=mod, outdir=mod.outdir, save_mode=False, show_mode=False)\n",
    "        name = mod.recurrent_layer.__name__\n",
    "    else:\n",
    "        name = mod.__class__.__name__\n",
    "        plotter = Plotter(data=data, model=mod, outdir=mod.outdir, save_mode=False, show_mode=False)\n",
    "    \n",
    "    # Calculate the measures per status group for each label (only 1)\n",
    "    for p in pred:\n",
    "        values = plotter.measures_per_status('test', p).values\n",
    "        values = np.hstack([values, np.vstack([name] * values.shape[0])])\n",
    "        all_values.append(values)\n",
    "del(plotter)\n",
    "\n",
    "# Create pandas dataframe of the evaluation metrics\n",
    "performance_status = pd.DataFrame(np.concatenate(all_values),\n",
    "                                  columns=['status', 'measure', 'value', 'model'])\n",
    "performance_status = performance_status.replace(to_replace=r'RandomForest.+', value='RF', regex=True)\n",
    "performance_status = performance_status.replace(to_replace=r'.+SVM.+', value='SVM', regex=True)\n",
    "performance_status.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define correct measure names\n",
    "if prob[0] == 'regression':\n",
    "    measures = ['MAE', 'RMSE', 'Max AE', 'R2']\n",
    "else:\n",
    "    measures = ['AUC', 'F1', 'Accuracy']\n",
    "\n",
    "# Use dict to order models\n",
    "custom_dict = {'CNN': 0, 'LSTM': 1, 'GRU': 2, 'SimpleRNN': 3, 'NN': 4, 'RF': 5, 'SVM': 6, 'Constant': 7}\n",
    "    \n",
    "# Figure and axes\n",
    "fig, axs = plt.subplots(len(measures), 1, figsize=(5.972, 8.67 * .8), sharex=True, sharey=False)\n",
    "for i, m, ax in zip(range(len(measures)), measures, axs):\n",
    "    # Barplot each measure (m) at axis i\n",
    "    sns.barplot(x='status', y='value', hue='model', dodge=True,\n",
    "                data=performance_status.loc[performance_status['measure'] == m]\\\n",
    "                                          .sort_values(by=['model'], key=lambda x: x.map(custom_dict)),\n",
    "                ci=100, errwidth=1.5, capsize=.05,\n",
    "                estimator=min if m != 'R2' and prob[0] == 'regression' else max,\n",
    "                ax=ax)\n",
    "    ax.get_legend().remove()\n",
    "    if i != len(measures) - 1:\n",
    "        ax.set_xlabel('')\n",
    "    ax.set_ylabel(m)\n",
    "    \n",
    "    # Set x and y lim\n",
    "    min_val = np.inf\n",
    "    max_val = -np.inf\n",
    "    for p in ax.patches:\n",
    "        val = p.get_height()\n",
    "        if val < min_val:\n",
    "            min_val = val\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "    ax.set_ylim((min_val * 0.95, max_val * 1.01))\n",
    "\n",
    "# Set legend and save plots\n",
    "axs[1].legend(loc=7, bbox_to_anchor=(1.3, 0.5), prop={\"size\":8})\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(right=0.8)\n",
    "plt.savefig('results/figures/status_performance_{}.pdf'.format(pred[0]))\n",
    "plt.savefig('results/figures/status_performance_{}.png'.format(pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best models, test MAE the lowest or test AUC the highest\n",
    "if prob[0] == 'regression':\n",
    "    best_models = df.loc[df.groupby('model_type')[measure_labels[0]].transform('min') == df[measure_labels[0]]]\n",
    "    best_models = best_models.sort_values(by=measure_labels)\n",
    "else:\n",
    "    best_models = df.loc[df.groupby('model_type')[measure_labels[0]].transform('max') == df[measure_labels[0]]]\n",
    "    best_models = best_models.sort_values(by=measure_labels, ascending=False)\n",
    "best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Find for each tuned model the best model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Use correct wrapper with correct name\n",
    "if prob[0] == 'regression':\n",
    "    name_to_model = {'RF': RandomForestReg, 'SVM': LinearSVMReg, 'Constant': Constant, 'GRU': RNN, 'LSTM': RNN,\n",
    "                     'CNN': CNN, 'NN': NN, 'SimpleRNN': RNN}\n",
    "else:\n",
    "    name_to_model = {'RF': RandomForestClass, 'SVM': LinearSVMClass, 'Constant': Constant, 'GRU': RNN,\n",
    "                     'LSTM': RNN, 'CNN': CNN, 'NN': NN, 'SimpleRNN': RNN}\n",
    "    \n",
    "model_to_name = dict((v,k) for k, v in name_to_model.items())\n",
    "\n",
    "# Select models based on best_models variable\n",
    "selected_models = []\n",
    "for name, model_dir, model in zip(best_models['model_type'].values, best_models['model_dir'].values,\n",
    "                                  best_models['model'].values):\n",
    "    if name in ['SimpleRNN', 'LSTM', 'GRU']:\n",
    "        selected_models.append(name_to_model[model](data=RNN_data, outdir=join(out, name, model_dir), load_from_dir=True))\n",
    "    elif name != 'Constant':\n",
    "        selected_models.append(name_to_model[model](data=data, outdir=join(out, name, model_dir), load_from_dir=True))\n",
    "    else:\n",
    "        selected_models.append(Constant(data=data, outdir=out, load_from_dir=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Out of all best tuned models, take the best model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best found model for each label\n",
    "best_model_dict = {'drive': 'GRU', 'cUHDRS': 'GRU', 'motscore': 'CNN', 'tfcscore': 'GRU',\n",
    "                   'sdmt1': 'GRU', 'swrt1': 'GRU'\n",
    "                  }\n",
    "target_model = best_model_dict[pred[0]]\n",
    "\n",
    "# loop through all models that were the best of the type\n",
    "for mod in selected_models:\n",
    "    # If the models is the one that is the best the name\n",
    "    if (target_model == mod.__class__.__name__ or\n",
    "        (mod.__class__.__name__ == 'RNN' and mod.recurrent_layer.__name__ == target_model)):\n",
    "        best_mod = mod\n",
    "        break\n",
    "\n",
    "# Print the file of the best model\n",
    "print(best_mod.__class__.__name__, best_mod.outdir)\n",
    "with open('results/best_model_{}'.format(pred[0]), 'w') as outfile:\n",
    "    outfile.write('{}\\t{}\\t{}\\n'.format(pred[0], best_mod.__class__.__name__, best_mod.outdir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31629b5f1f0da8e48893f7ca130076326da2a6d9b1f4e4c551ba1798dcd15a43"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('hd_paper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
