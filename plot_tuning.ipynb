{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from os.path import join, isfile, isdir\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from models.RNN import RNN\n",
    "\n",
    "from plot import Plotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here different labels can be selected to show performance\n",
    "\n",
    "pred = ['drive']\n",
    "prob = ['classification']\n",
    "out = join('output', '_'.join(pred), 'tuning')\n",
    "\n",
    "# Name of measure labels\n",
    "if prob[0] == 'regression':\n",
    "    measure_labels = ['test_{}_{}'.format(pred[0], m) for m in ['MAE', 'RMSE', 'Max AE', 'R2']]\n",
    "else:\n",
    "    measure_labels = ['test_{}_{}'.format(pred[0], m) for m in ['AUC', 'F1', 'Accuracy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve logs and performance (metrics appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make one big pd dataframe of the tuned arguments and models\n",
    "# This makes the appendix of all the variable metrics\n",
    "\n",
    "performance = pd.DataFrame([])\n",
    "values = []\n",
    "\n",
    "# Retrieve log for each model type\n",
    "for model_type in ['GRU_RNN']: #  ['RandomForestClass', 'RandomForestReg', 'LinearSVMClass', 'LinearSVMReg', 'Constant', 'GRU_RNN', 'NN', 'LSTM_RNN', '1d_CNN', 'SimpleRNN_RNN']:\n",
    "    # Model performances\n",
    "    if not isdir(join(out, model_type)):\n",
    "        print(join(out, model_type), 'not found')\n",
    "        continue\n",
    "    # Read log\n",
    "    log = pd.read_csv(join(out, model_type, 'log.csv')) # , usecols=['model'] + measure_labels)\n",
    "    log['model_type'] = model_type\n",
    "    # Concatenate log with df\n",
    "    performance = pd.concat([performance, log])\n",
    "    \n",
    "    # Loop through all trained models\n",
    "    for trained_models in glob.glob(join(out, model_type, 'model*')):\n",
    "        for model in trained_models.split('\\n'):\n",
    "            model_name = os.path.split(model)[-1]\n",
    "            if isfile(join(model, 'train_args.pickle')):\n",
    "                # Get training parameters of model to change some columns names\n",
    "                # Also to show which different hyperparameters are tuned\n",
    "                model_args = pickle.load(open(join(model, 'train_args.pickle'), 'rb'))\n",
    "                \n",
    "                # If recurrent specify which one SimpleRNN, LSTM, or GRU\n",
    "                if 'recurrent_layer' in model_args.keys():\n",
    "                    model_args['model_type'] = model_args['recurrent_layer'].__name__ + '_RNN'\n",
    "                    del(model_args['recurrent_layer'])\n",
    "                else:\n",
    "                    model_args['model_type'] = model_type\n",
    "                \n",
    "                # Change n_filters and hiddensize parameter to combined column (same thing)\n",
    "                if 'n_filters' in model_args.keys():\n",
    "                    model_args['hidden_size/n_filters'] = model_args['n_filters']\n",
    "                    del(model_args['n_filters'])\n",
    "                elif 'hidden_size' in model_args.keys():\n",
    "                    model_args['hidden_size/n_filters'] = model_args['hidden_size']\n",
    "                    del(model_args['hidden_size'])\n",
    "            \n",
    "            # Baseline model (no training args)\n",
    "            else:\n",
    "                model_args = {'model_type': model_type}\n",
    "            \n",
    "            model_args['model'] = model_name\n",
    "            values.append(model_args)\n",
    "\n",
    "# Show pandas dataframe of trained models evaluation metrics and the information in the log file\n",
    "df = pd.DataFrame(values)\n",
    "df = df.merge(performance, on=['model', 'model_type'])\n",
    "df = df.rename(columns={'model': 'model_dir'})\n",
    "# df = df.loc[df['layers'] != 5] # Remove layer=5, only trained for some\n",
    "# df = df.drop(['model'], axis='columns')\n",
    "# df = df.loc[:, ((~df.isnull().all()) & (df.nunique() != 1))]\n",
    "\n",
    "# Rename model type to more interpretaable name\n",
    "type_to_name = {'RandomForestClass': 'RF', 'RandomForestReg': 'RF',\n",
    "                'LinearSVMClass': 'SVM', 'LinearSVMReg': 'SVM', 'Constant': 'Constant', 'NN': 'NN',\n",
    "                'GRU_RNN': 'GRU', 'LSTM_RNN': 'LSTM', '1d_CNN': 'CNN', 'SimpleRNN_RNN': 'SimpleRNN'}\n",
    "df['model'] = df['model_type'].apply(lambda x: type_to_name[x])\n",
    "\n",
    "df.groupby('model').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Make the appendix </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort the big dataframe by the first measure label (MAE/AUC) to define best model\n",
    "stats_all_models = df.sort_values(by=measure_labels[0])\\\n",
    "        .loc[:, ~df.columns.isin(['model_type', 'model_dir'])]\n",
    "\n",
    "# Reformat floats to .3 precision\n",
    "stats_all_models['L2'] = stats_all_models['L2'].map('{:.0e}'.format)\n",
    "# stats_all_models['layers'] = stats_all_models['layers'].map('{:g}'.format)\n",
    "stats_all_models['hidden_size/n_filters'] = stats_all_models['hidden_size/n_filters'].map('{:g}'.format)\n",
    "if 'n_estimators' in stats_all_models.columns:\n",
    "    stats_all_models['n_estimators'] = stats_all_models['n_estimators'].map('{:g}'.format)\n",
    "if 'epsilon' in stats_all_models.columns:\n",
    "    stats_all_models['epsilon'] = stats_all_models['epsilon'].map('{:.0e}'.format)\n",
    "\n",
    "# For each measure (MAE, RMSE, etc.) reformat precision of float to .3\n",
    "# Also change column names\n",
    "for m in measure_labels:\n",
    "    stats_all_models[m] = stats_all_models[m].map('{:.3f}'.format)\n",
    "    new_m = m.split('_')\n",
    "    new_m[0] = 'train'\n",
    "    new_m = '_'.join(new_m)\n",
    "    stats_all_models[new_m] = stats_all_models[new_m].map('{:.3f}'.format)\n",
    "    stats_all_models = stats_all_models.rename(columns={m: 'test ' + m.split('_')[-1],\n",
    "                                                        new_m: 'train ' + new_m.split('_')[-1]})\n",
    "\n",
    "# Replace nans with -, looks nicer\n",
    "stats_all_models = stats_all_models.replace(np.nan, '-')\n",
    "stats_all_models = stats_all_models.replace('nan', '-')\n",
    "# Drop some uninformative info\n",
    "stats_all_models = stats_all_models.drop(['epochs', 'best epoch', 'elapsed time'], axis='columns')\n",
    "\n",
    "# Sort the dataframe by model, test MAE for regression\n",
    "# Sort the dataframe by model, test AUC, test F1, test Accuracy\n",
    "stats_all_models = stats_all_models.sort_values(by=['model', 'test MAE'] if prob[0] == 'regression'  \n",
    "                                                    else ['model', 'test AUC', 'test F1', 'test Accuracy'],\n",
    "                                                ascending=(True if prob[0] == 'regression' else False)\n",
    "                                               )\n",
    "stats_all_models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save appendix\n",
    "stats_all_models.drop(['bidirect', 'drop', 'rdrop'], axis=1)\\\n",
    "                .to_csv('additional_files/Additional file 6.csv'.format(pred[0]), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ed85de37ebc6aa7d5256bd6e2a558a30ac7413d85458a0b33d66c485fa854fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
